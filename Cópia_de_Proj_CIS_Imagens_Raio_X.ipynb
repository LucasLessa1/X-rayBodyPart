{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LucasLessa1/X-rayBodyPart/blob/davi1/C%C3%B3pia_de_Proj_CIS_Imagens_Raio_X.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lU48iA-hf04s"
      },
      "source": [
        "#Imports and Ajusting file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Proj_CIS_Imagens_Raio_X.ipynb"
      ],
      "metadata": {
        "id": "vdP-BQTpJ_qP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KY-thcKqQhmX",
        "outputId": "36c1f5d1-39b5-474b-a693-0a1a08dff163"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pydicom in /usr/local/lib/python3.7/dist-packages (2.3.1)\n"
          ]
        }
      ],
      "source": [
        "#A instalação da biblioteca pydicom \n",
        "#Ela trata da leitura de arquivos de imagens de exames de Raio-X, que são feitos em formatos DCM \n",
        "\n",
        "!pip install pydicom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "X75kHVXu3C8m"
      },
      "outputs": [],
      "source": [
        "###########################################################################\n",
        "#           Importanção de todos os pacotes utilizados                    # \n",
        "###########################################################################\n",
        "\n",
        "import cv2            #Tratamento de imagem \n",
        "import pydicom         #Tratamento de arquivo DCM \n",
        "import pandas as pd     #Préprocessamento em tabelas\n",
        "import os                #Pacote de tarefas de sistema operavional\n",
        "import shutil            #Complementar à anterior\n",
        "import time               #Medir tempo de execução \n",
        "import random              #Pacote para escolha de números de forma pseudo-aleatória \n",
        "import math                #Matemática \n",
        "import gdown                #Download de direitório salvo no Google Drive \n",
        "import zipfile               #Tratamento de arquivos em pasta zipadas\n",
        "import numpy as np            #Operações numéricas e matemáticas em 'arrays'\n",
        "from scipy.io import loadmat \n",
        "from matplotlib import pyplot     \n",
        "import matplotlib.pyplot as plt   #Visualização de arquivo \n",
        "from google.colab.patches import cv2_imshow    #Visualição de arquivo \n",
        "from tqdm import tqdm     #Barra de progresso\n",
        "from PIL import Image     #Operações com imagens \n",
        "import string             \n",
        "import glob             #Pacote de operações com múltiplos arquivos que possuam nome ou extensão em comum\n",
        "\n",
        "\n",
        "from numpy import expand_dims    #Expansão de dimensões do \"shape\" (formato) da imagem\n",
        "import tensorflow as tf          #Operações de vetores para o modelo de aprendizado profundo \n",
        "from tensorflow.keras.utils import load_img\n",
        "from tensorflow.keras.utils import img_to_array\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "sqf48VqoeCqt"
      },
      "outputs": [],
      "source": [
        "###########################################################################\n",
        "#           Download e Extração dos arquivos originais                    # \n",
        "###########################################################################\n",
        "\n",
        "#Funções:\n",
        " \n",
        "def download(id): \n",
        "    url = 'https://drive.google.com/uc?id=' + str(id)\n",
        "    gdown.download(url, output = None, quiet = False)\n",
        "\n",
        "def unzip(path): #Função para unzip\n",
        "    zip = zipfile.ZipFile(path)\n",
        "    zip.extractall()\n",
        "    zip.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEFniMhVgyUh",
        "outputId": "2b5b4d46-30b0-4786-a299-2aeae30303fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1ev-r31j8oRzDlKM_toaeADO2psrA_XXm\n",
            "To: /content/archive.zip\n",
            "100%|██████████| 269M/269M [00:01<00:00, 187MB/s]\n"
          ]
        }
      ],
      "source": [
        "#Chamando funções de Download e Extração \n",
        "\n",
        "# https://drive.google.com/file/d/1ev-r31j8oRzDlKM_toaeADO2psrA_XXm/view?usp=sharing\n",
        "\n",
        "download('1ev-r31j8oRzDlKM_toaeADO2psrA_XXm')\n",
        "unzip('/content/archive.zip')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "PpL-tSErjuD6"
      },
      "outputs": [],
      "source": [
        "#leitura dos arquivos originais para obter tanto o dataset de treino quanto o dataset de teste, que servirá de prova real\n",
        "\n",
        "#os.chdir(\"/content/drive/MyDrive/RAIO-X/RAIO-X/archive.zip (Unzipped Files)\")\n",
        "path = os.getcwd() \n",
        "train_df = pd.read_csv('/content/train.csv')\n",
        "test_df = pd.read_csv(os.path.join('/content/sample_submission.csv'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "c8qRth9FkA5p",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8b233041-89ca-463b-8a5b-a048a990ee91"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      SOPInstanceUID  \\\n",
              "0  1.2.826.0.1.3680043.8.498.10001001190452685542...   \n",
              "1  1.2.826.0.1.3680043.8.498.10022667601042710442...   \n",
              "2  1.2.826.0.1.3680043.8.498.10024395388921105474...   \n",
              "3  1.2.826.0.1.3680043.8.498.10026689165626095651...   \n",
              "4  1.2.826.0.1.3680043.8.498.10035936364561920980...   \n",
              "\n",
              "                                              Target  \n",
              "0  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
              "1  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
              "2  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
              "3  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  \n",
              "4  0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-71293982-9035-4e65-b3d3-dbe49721be58\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>SOPInstanceUID</th>\n",
              "      <th>Target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.10001001190452685542...</td>\n",
              "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.10022667601042710442...</td>\n",
              "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.10024395388921105474...</td>\n",
              "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.10026689165626095651...</td>\n",
              "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.2.826.0.1.3680043.8.498.10035936364561920980...</td>\n",
              "      <td>0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-71293982-9035-4e65-b3d3-dbe49721be58')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-71293982-9035-4e65-b3d3-dbe49721be58 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-71293982-9035-4e65-b3d3-dbe49721be58');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ],
      "source": [
        "test_df.head(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qd9hejxZkD94"
      },
      "source": [
        "#Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "0IQpmWP3kIhQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8e3e3068-bbeb-4ed3-d972-c933e8a4c5ed"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Columns:\n",
            "Index(['SOPInstanceUID', 'Target'], dtype='object') \n",
            "\n",
            "Types in columns:\n",
            "SOPInstanceUID    object\n",
            "Target            object\n",
            "dtype: object \n",
            "\n",
            "Types in info:\n",
            "                                           SOPInstanceUID Target\n",
            "count                                                1738   1738\n",
            "unique                                               1738     41\n",
            "top     1.2.826.0.1.3680043.8.498.10025629581362719970...     3 \n",
            "freq                                                    1    724 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Confere propriedades de formato e conteudo do Pandas Dataset \n",
        "print(\"Columns:\")\n",
        "print(train_df.columns, \"\\n\")\n",
        "\n",
        "print(\"Types in columns:\")\n",
        "print(train_df.dtypes, \"\\n\")\n",
        "\n",
        "print(\"Types in info:\")\n",
        "print(train_df.describe(), \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Distribuição das labels em formato de dicionário com a chave sendo número e o valor sendo o nome da label em texto \n",
        "\n",
        "bodyparts = {\n",
        "0 : 'Abdomen' ,\n",
        "1 :'Ankle' ,\n",
        "2 :'Cervical Spine',\n",
        "3 : 'Chest' ,\n",
        "4 :'Clavicles' ,\n",
        "5 :'Elbow' ,\n",
        "6 :'Feet' ,\n",
        "7 : 'Finger' ,\n",
        "8 : 'Forearm' ,\n",
        "9 : 'Hand' ,\n",
        "10 : 'Hip' ,\n",
        "11 : 'Knee' ,\n",
        "12 : 'Lower Leg' ,\n",
        "13 : 'Lumbar Spine' ,\n",
        "14 : 'Others' ,\n",
        "15 :'Pelvis',\n",
        "16 :'Shoulder' ,\n",
        "17 :'Sinus' ,\n",
        "18 : 'Skull' ,\n",
        "19 : 'Thigh' ,\n",
        "20 :'Thoracic Spine',\n",
        "21: 'Wrist',\n",
        "}"
      ],
      "metadata": {
        "id": "YCh3lR7WCsME"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hk9Kily5l8fI"
      },
      "source": [
        "##Fixing folders"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = []"
      ],
      "metadata": {
        "id": "8nxIWWkloUir"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "0Z0LwdkVoI3l"
      },
      "outputs": [],
      "source": [
        "#Classe de processamento de imagem que mexe com pastas também -começa declarando variaveis de modificação de imagem \n",
        "class Image_processing():\n",
        "  def __init__(self, folder, newFolder):\n",
        "    self.fileList = []\n",
        "    self.rotationRange = 90 \n",
        "    self.brightRange = [0.2, 1.5]\n",
        "    self.folder = folder #This is the folder of the original folder image\n",
        "    self.newFolder = newFolder #This is the new folder\n",
        "\n",
        "#constroi pastas para cada label\n",
        "  def createfolders(self):\n",
        "    os.mkdir(f'/content/{self.newFolder}/') \n",
        "    #os.chdir(f'/content/{self.newFolder}') \n",
        "    for bodypart in list(bodyparts.values()):\n",
        "      if os.path.isdir(f'/content/{self.newFolder}/{bodypart}') == False:\n",
        "        os.mkdir(f'/content/{self.newFolder}/{bodypart}')\n",
        "      else:  \n",
        "          pass       \n",
        "#Coloca cada imagem do arquivo original \"train/train/train\" em uma pasta especifica de cada label para fins de treino do modelo \n",
        "  def folderImagebyLabel(self, df, dictionary):\n",
        "    filelist = []\n",
        "    #anda por todos os diretorios e arquivos de \"train/train/train\"\n",
        "    for root, dirs, files in os.walk(f'/content/{str(self.folder)}'):\n",
        "      for file in files:\n",
        "        filelist.append(os.path.join(root,file))\n",
        "\n",
        "    for filename in filelist:\n",
        "      #Lê os arquivos de formato DCM por meio da biblioteca apropriada\n",
        "      dicom = pydicom.dcmread(filename)\n",
        "      #Transforma a imagem em numpy array \n",
        "      img = dicom.pixel_array\n",
        "      #Separa seu nome por cada barra - o nome sendo identificador_de_estudo/identificador_de_serie_de_exames/identificador_unico_de_imagem \n",
        "      var = filename.split(\"/\")\n",
        "      #Pega o último termo que foi separado - identificador unico\n",
        "      var = var[-1]\n",
        "      #Vai atrás do arquivo no dataframe formado para econtrar sua label específica; omiti deste identficador o \"-c.dcm\"(extensão do arquivo)\n",
        "      row = df.index[df['SOPInstanceUID']==f'{var[:-6]}'].tolist()\n",
        "      #Guarda a label\n",
        "      target = df['Target'].iloc[row[0]]\n",
        "      #Aqui nós arbitrariamente não utilizamos imagens com mais de uma label para não prejudicar o aprendizado do modelo\n",
        "      if len(target.strip()) > 2: \n",
        "        continue\n",
        "      #Encontra o nome da label baseada no número da coluna do dataframe conferindo no dicionário já feito\n",
        "      label = dictionary.get(int(target))\n",
        "      #Passo importante de formatar a imagem numa amplitude de pixels comumente aceita nas funções de processamento de imagem \n",
        "      resized_img  = (np.maximum(img,0)/img.max())*255 \n",
        "      #Transforma em formato 8 bits pelo mesmo motivo \n",
        "      im = resized_img.astype(np.uint8)\n",
        "      # Chama classe CLAHE(equaliador de brilho) -argumentos: valor máximo para limitação do contraste(evitar predominancia de sinal ruim), tamanho dos grides de divisão de imagem para se aplicar o processo\n",
        "      clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "      # Aplica no canal que importa- a luminosidade\n",
        "      clahe_img = clahe.apply(im)\n",
        "      #Volta a colocá-lo em numpy array \n",
        "      im = Image.fromarray(clahe_img)\n",
        "      im.show()\n",
        "      #Salva imagem em formato PNG \n",
        "      im.save(os.path.join(f'/content/{self.newFolder}', f'{label}', f'{var[:-6]}.png'))\n",
        "\n",
        "  #Este método coloca todas as imagens do diretório \"train/train/train\" misturada em um pasta só para fins de testagem do modelo (mesmo processo)\n",
        "  def allImagesFolder(self, df):\n",
        "    filelist = []\n",
        "    for root, dirs, files in os.walk(f'/content/{str(self.folder)}'):\n",
        "      for file in files:\n",
        "        filelist.append(os.path.join(root,file))\n",
        "\n",
        "    for filename in filelist:\n",
        "      dicom = pydicom.dcmread(filename)\n",
        "      img = dicom.pixel_array\n",
        "      var = filename.split(\"/\")\n",
        "      var = var[-1]\n",
        "      row = df.index[df['SOPInstanceUID']==f'{var[:-6]}'].tolist()\n",
        "      resized_img  = (np.maximum(img,0)/img.max())*255 \n",
        "      im = resized_img.astype(np.uint8)\n",
        "      clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8))\n",
        "      clahe_img = clahe.apply(im)\n",
        "      im = Image.fromarray(clahe_img)\n",
        "      im.show()\n",
        "      im.save(os.path.join(f'/content/{self.newFolder}/', f'{var[:-6]}.png'))\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Organize the test images folder\n",
        "# 856vbnm\n",
        "nameFolderTest = \"test_img\"\n",
        "#Chama a classe\n",
        "processImageTest = Image_processing(folder='test', newFolder = nameFolderTest)\n",
        "\n",
        "#Testa se seus metodos de criação de pasta e modificação de arquivos já foram rodados, se não o código as rodará agora para formar a pasta de teste \n",
        "try:\n",
        "  os.mkdir(f'/content/{nameFolderTest}')\n",
        "  processImageTest.allImagesFolder(train_df)\n",
        "\n",
        "except:\n",
        "  shutil.rmtree(f'/content/{nameFolderTest}/')\n",
        "  os.mkdir(f'/content/{nameFolderTest}')\n",
        "  processImageTest.allImagesFolder(train_df)\n"
      ],
      "metadata": {
        "id": "v45QZzKw_cTc"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Organize the train images folder\n",
        "nameFolderTrain = \"train_img\"\n",
        "processImageTrain = Image_processing(folder='train', newFolder = nameFolderTrain)\n",
        "\n",
        "#Testa se seus metodos de criação de pasta e modificação de arquivos já foram rodados, se não o código as rodará agora para formar a pasta de treino\n",
        "try:\n",
        "  processImageTrain.createfolders()\n",
        "  #This \"train\" is the name of the original folder of the images\n",
        "  processImageTrain.folderImagebyLabel(train_df, bodyparts)\n",
        "except:\n",
        "  shutil.rmtree(f'/content/{nameFolderTrain}/')\n",
        "  processImageTrain.createfolders()\n",
        "  processImageTrain.folderImagebyLabel(train_df, bodyparts)\n"
      ],
      "metadata": {
        "id": "n4V9OW0-PdNf"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Classe do data augmentation propriamente dito que vai herdar as propriedades da classe anterior \n",
        "class Data_Augumentation(Image_processing):\n",
        "\n",
        "  def __init__(self, folder, newFolder, folder_img):\n",
        "    #super().__init__() chama as propriedades da classe superior ao colocar os argumentos necessários para rodar aquela classe\n",
        "    super().__init__(folder, newFolder)\n",
        "    # Image_processing.__init__(self, folder, newFolder)\n",
        "    self.folder_img = folder_img\n",
        "\n",
        " #Este método cria imagens artificias baseada na necessidade de mais imagens a fim de se ter o mesmo número de imagens requisitadas em cada label\n",
        "  def process_imgs(self, qntd = 100):\n",
        "    for folder in os.listdir(f'/content/{self.folder_img}'):\n",
        "      #real_qntd é a quantidade de imagens de exames reais\n",
        "      real_qntd = len(os.listdir(f'/content/{self.folder_img}/{folder}'))\n",
        "      diff = qntd - real_qntd\n",
        "      #Aqui se compara se temos mais imagens do que precisamos ou menos \n",
        "      if qntd > real_qntd: \n",
        "        #A quantidade que temos tem que ser maior do que a diferença\n",
        "        #Porque num fator de uma imagem artificial para uma original conseguimos apenas dobrar a quantidade original \n",
        "        if real_qntd >= diff:\n",
        "          #exams  = lista de arquivos sorteados de imagens para cara pasta de label \n",
        "          exams = random.choices(os.listdir(f'/content/{self.folder_img}/{folder}'), k= diff)\n",
        "          self.augment(folder, exams, 1)\n",
        "        else:\n",
        "          #Precisamos de imagens adicionais em quantas vezes mais do que já temos?\n",
        "          for i in range(100):\n",
        "            check = real_qntd + real_qntd*i\n",
        "            if check > qntd: \n",
        "              break\n",
        "            multiplier = i\n",
        "            diff_mult = qntd - check\n",
        "            \n",
        "          #De cada uma imagem original vamos pegar x vezes para aproximar do requerido \n",
        "          exams = [x for x in os.listdir(f'/content/{self.folder_img}/{folder}') if x.find(\"copy\") == -1 ]\n",
        "          self.augment(folder, exams, multiplier)\n",
        "          #Aqui pegamos por sorteio o resto que escapa do multiplo\n",
        "          if diff_mult>0:\n",
        "            exams = [x for x in os.listdir(f'/content/{self.folder_img}/{folder}') if x.find(\"copy\") == -1 ]\n",
        "            exams = random.choices(exams, k= diff_mult)\n",
        "            self.augment(folder, exams, 1)\n",
        "          \n",
        "          \n",
        "      #Caso se tenha mais imagens originais do que o requisitado, nos desfazemos das imagens em excesso por meio de sorteio também\n",
        "      elif real_qntd > qntd:\n",
        "        diff = (real_qntd-qntd)\n",
        "        sorteados = glob.glob(f'/content/{self.folder_img}/{folder}/*')\n",
        "        for s in random.sample(sorteados, diff):\n",
        "          os.remove(s)\n",
        "\n",
        "\n",
        "  def augment(self, folder, exams, limit):\n",
        "    #folder = nome que eu dei para pasta da label (ex : skull)\n",
        "    #exams  = lista de arquivos de imagens para cara pasta de label \n",
        "    #limit = quantidade de imagens artificias criadas a partir da original\n",
        "    \n",
        "    nameImage = []\n",
        "    cont1 = -1\n",
        "    for exam in exams: \n",
        "      #Transfere os elementos de uma lista para outra para fins de organização\n",
        "      nameImage.append(exam)\n",
        "      #Carrega a imagem \n",
        "      img = load_img(f'/content/{self.folder_img}/{folder}/{exam}')\n",
        "      #Transforma em numpy array \n",
        "      data = img_to_array(img)\n",
        "      #Acrescenta uma dimenção no \"shape\" da imagem referente ao batch - que será tamanho 1, o que não muda nada e serve apenas para rodar a função\n",
        "      samples = expand_dims(data, 0)\n",
        "      #Chama gerador de imagens utilizando dos argumentos de alteração de imagens - superficiais, mas o suficiente para confundir o modelo de aprendizado...\n",
        "      #... e semelhante o suficiente para não desfazer os formatos essenciais de partes do corpo humano que serão analisadas \n",
        "      datagen = ImageDataGenerator(horizontal_flip=True,rotation_range = self.rotationRange, brightness_range = self.brightRange)\n",
        "      #Prepara o iterador \n",
        "      it = datagen.flow(samples, batch_size=1)\n",
        "      cont1 =+ 1\n",
        "      #Esta variável é uma \"compreensão de lista\" que verifica se a imagem não é uma duplicata ao olhar a lista \"nameImage\", que serve de histórico\n",
        "      dup = [x for i, x in enumerate(nameImage) if i != nameImage.index(x)]\n",
        "\n",
        "      #Em caso de duplicata, é colocado é escrito em seu nome que trata-se de repetição e um identificador aleatório \n",
        "      if True:\n",
        "        equalDup = \"_repeted_\" + str(random.randint(-9999,9999))\n",
        "      else:\n",
        "        equalDup = \"\"\n",
        "        \n",
        "      for cont in range(limit):\n",
        "        # Aqui trata-se do processo de adequação da imagem nos formatos necessários e salvá-la com nome único para evitar sobrescrição dos arquivos\n",
        "        batch = it.next()\n",
        "        image = batch[0].astype('uint16')\n",
        "        resized_img  = (np.maximum(image,0)/image.max())*255 \n",
        "        im = Image.fromarray(resized_img.astype(np.uint8))\n",
        "        rand = random.randint(-9999,9999)\n",
        "        info.append(f'/content/{self.folder_img}/{folder}/{exam[:-4]}_copy_{cont1}_{cont}.png')\n",
        "        im.save(f'/content/{self.folder_img}/{folder}/{exam[:-4]}_{equalDup}_copy_{cont1}_{cont}.png')\n"
      ],
      "metadata": {
        "id": "Gsed_3Jp4kdA"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Augumented for Train image\n",
        "\n",
        "#Esta função ja pode ser chamada sem muita discrição por conseguir se repetir sem deixar rastros, diferentes das prévias que mexem com pastas\n",
        "AugumentedTrain  =  Data_Augumentation(folder='train', newFolder = nameFolderTrain, folder_img = \"train_img\")\n",
        "AugumentedTrain.process_imgs()\n",
        "\n",
        "# #Augumented for Train image\n",
        "# AugumentedTest  =  Data_Augumentation(folder='test', newFolder = nameFolderTest, folder_img = \"test_img\")\n",
        "# AugumentedTest.process_imgs()"
      ],
      "metadata": {
        "id": "UoiUwPBiWc87"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for files in os.listdir('/content/train_img'):\n",
        "  print(files)\n",
        "  print(len(os.listdir(f'/content/train_img/{files}')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZjnXVpK5erpo",
        "outputId": "e45267f0-8875-45a8-ee2f-b7e4858498f4"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Others\n",
            "100\n",
            "Shoulder\n",
            "100\n",
            "Ankle\n",
            "100\n",
            "Clavicles\n",
            "100\n",
            "Thigh\n",
            "100\n",
            "Chest\n",
            "100\n",
            "Lower Leg\n",
            "100\n",
            "Knee\n",
            "100\n",
            "Abdomen\n",
            "100\n",
            "Sinus\n",
            "100\n",
            "Elbow\n",
            "100\n",
            "Hand\n",
            "100\n",
            "Feet\n",
            "100\n",
            "Forearm\n",
            "100\n",
            "Wrist\n",
            "100\n",
            "Hip\n",
            "100\n",
            "Thoracic Spine\n",
            "100\n",
            "Finger\n",
            "100\n",
            "Cervical Spine\n",
            "100\n",
            "Pelvis\n",
            "100\n",
            "Lumbar Spine\n",
            "100\n",
            "Skull\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Esta classe irá criar outro diretório para todos os arquivos serem colocados juntos \n",
        "class SelectFolders():\n",
        "#Declaram-se as variáveis iniciais baseados em caminhos de pasta e quantidade de imagens requeridas por input \n",
        "  def __init__(self, mainPath, root, qntd):\n",
        "    self.mainPath = mainPath\n",
        "    self.qntdImages = qntd\n",
        "    self.listNames = []\n",
        "    self.flag = []\n",
        "    self.rootPath = root\n",
        "  \n",
        "  #Verifica a quantidade de cada arquivo em cada pasta \n",
        "  def verify(self, folder2verify, order):\n",
        "    for files in os.listdir(folder2verify):\n",
        "      self.listNames.append([files, len(os.listdir(f'{folder2verify}/{files}')) ])\n",
        "      #Verifica lista listNames que possui os nomes das pastas de labels e o numeros de arquivos nelas\n",
        "\n",
        "    for i in self.listNames:\n",
        "      if i[1] > self.qntdImages:\n",
        "        #False se tiver pastas com menos de 100 imagens ou menos que self.qntdImages\n",
        "        #A lista flag quarda o valor de falos, e em seguida o nome da pasta da label e o numero de imagens nele \n",
        "        self.flag.append([False, i[0], i[1]])\n",
        "    #Printa a lista se requisitado \n",
        "    if order == \"print\":\n",
        "      print(self.flag)\n",
        "    if order ==\"result\":\n",
        "      flagzinha = False\n",
        "      #zip faz uma tupla de tuplas de todos o primeiros, segundos e terceiros elementos \n",
        "      for i in zip(self.flag): \n",
        "        if i[0] ==True:\n",
        "          print(f\"Error: The folder {i[1]} has {i[2]} Images.\")\n",
        "        else:\n",
        "          flagzinha = True\n",
        "\n",
        "      if flagzinha == True: \n",
        "        print(f\"Every folder has more than {self.qntdImages} Images.\")\n",
        "\n",
        "  #Este metódo constrói pasta uma pasta com pasta de labels a semelhança do que já foi feito \n",
        "  #Mas antes será verificado se isso já não foi feito \n",
        "  def folder2Model(self, path, qntd , bodyparts): \n",
        "    try:\n",
        "      os.mkdir(f\"{self.rootPath}/{path}\")\n",
        "      print(self.rootPath)\n",
        "      print(path)\n",
        "    except:\n",
        "      shutil.rmtree(f\"{self.rootPath}/{path}\")\n",
        "      os.mkdir(f\"{self.rootPath}/{path}\")\n",
        "\n",
        "    #Para cada label em formato de texto é verificado se já existe uma pasta\n",
        "    for part in list(bodyparts.values()):\n",
        "      if os.path.isdir(f\"{self.rootPath}/{path}/{part}\") == False:\n",
        "        os.mkdir(f\"{self.rootPath}/{path}/{part}\")\n",
        "      #Seleciona 'x' imagens de uma pasta de label e copia para imageModel\n",
        "      selectImages = random.choices(os.listdir(f'{self.mainPath}/{part}'), k= qntd)\n",
        "\n",
        "      for image in selectImages:\n",
        "        pathImageModel = f\"{self.rootPath}/{path}/{part}\"\n",
        "        originalPathImages = f'{self.mainPath}/{part}/{image}'\n",
        "        # print(originalPathImages)\n",
        "        shutil.copy(originalPathImages, pathImageModel)\n"
      ],
      "metadata": {
        "id": "GyC4qq5UcHbf"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fix2 = SelectFolders(\"/content/train_img\", \"/content\" ,  100)\n",
        "path = \"imageModel/\"\n",
        "#Verify if train_img has more than 100 images \n",
        "fix2.verify( \"/content/train_img\" ,\"result\")\n",
        "\n",
        "#Organizing the folder for the model\n",
        "fix2.folder2Model(path, 100, bodyparts)\n",
        "\n",
        "#Verify if imageModel has 100 images for each folder \n",
        "print()\n",
        "fix2.verify(\"/content/imageModel\" ,\"result\")"
      ],
      "metadata": {
        "id": "AS05CxHYh7vW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1396edae-6eab-4f39-ad89-a9754fda5db5"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "e6MTXoa1i_w5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Selecionando tamanho da template\n",
        "batch_size = 32  \n",
        "img_height = 180 \n",
        "img_width = 180\n",
        "\n",
        "data_dir = \"/content/train_img\""
      ],
      "metadata": {
        "id": "Nhuc2rXIj1OB"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Constrói um tensorflow dataset para poder ser colocado no modelo da biblioteca Keras \n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  label_mode = 'categorical',\n",
        "  class_names = list(bodyparts.values()),\n",
        "  validation_split=0.2, #Parte destinada aum dataset de validação \n",
        "  subset=\"training\",\n",
        "  seed=123, #senha da randomização do processo \n",
        "  image_size=(img_height, img_width), #\"shape\" do arquivo\n",
        "  batch_size=batch_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MngoN3LFjbrX",
        "outputId": "8590b1ab-d16e-4ae0-9fdd-0d9ef5741057"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2200 files belonging to 22 classes.\n",
            "Using 1760 files for training.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(bodyparts.values())"
      ],
      "metadata": {
        "id": "gIwlq1FyrC_K",
        "outputId": "0f73e298-dccb-4829-896d-bc0c2d704a4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Abdomen',\n",
              " 'Ankle',\n",
              " 'Cervical Spine',\n",
              " 'Chest',\n",
              " 'Clavicles',\n",
              " 'Elbow',\n",
              " 'Feet',\n",
              " 'Finger',\n",
              " 'Forearm',\n",
              " 'Hand',\n",
              " 'Hip',\n",
              " 'Knee',\n",
              " 'Lower Leg',\n",
              " 'Lumbar Spine',\n",
              " 'Others',\n",
              " 'Pelvis',\n",
              " 'Shoulder',\n",
              " 'Sinus',\n",
              " 'Skull',\n",
              " 'Thigh',\n",
              " 'Thoracic Spine',\n",
              " 'Wrist']"
            ]
          },
          "metadata": {},
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Constrói um tensorflow dataset para poder ser colocado no modelo da biblioteca Keras \n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "  data_dir,\n",
        "  label_mode = 'categorical',\n",
        "  class_names = list(bodyparts.values()),\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  image_size=(img_height, img_width),\n",
        "  batch_size=batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vGpU9RawjgLh",
        "outputId": "37742ee9-7bb9-470d-cdd3-db56fd8573bc"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2200 files belonging to 22 classes.\n",
            "Using 440 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for files in os.listdir('/content/train_img'):\n",
        "  print(files)\n",
        "  print(len(os.listdir(f'/content/train_img/{files}')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fU6vecZz-1_X",
        "outputId": "26972ad4-57ed-4285-83b8-fdca1a791642"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Others\n",
            "100\n",
            "Shoulder\n",
            "100\n",
            "Ankle\n",
            "100\n",
            "Clavicles\n",
            "100\n",
            "Thigh\n",
            "100\n",
            "Chest\n",
            "100\n",
            "Lower Leg\n",
            "100\n",
            "Knee\n",
            "100\n",
            "Abdomen\n",
            "100\n",
            "Sinus\n",
            "100\n",
            "Elbow\n",
            "100\n",
            "Hand\n",
            "100\n",
            "Feet\n",
            "100\n",
            "Forearm\n",
            "100\n",
            "Wrist\n",
            "100\n",
            "Hip\n",
            "100\n",
            "Thoracic Spine\n",
            "100\n",
            "Finger\n",
            "100\n",
            "Cervical Spine\n",
            "100\n",
            "Pelvis\n",
            "100\n",
            "Lumbar Spine\n",
            "100\n",
            "Skull\n",
            "100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra"
      ],
      "metadata": {
        "id": "SHe_tRFokskR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = train_ds.class_names\n",
        "#print(class_names)"
      ],
      "metadata": {
        "id": "giL20nRqkjmB"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#print primeiras imagens (primeiro batch) do gerador \n",
        "plt.figure(figsize=(10, 10))\n",
        "for images, labels in train_ds.take(1):\n",
        "  for i in range(9):\n",
        "    ax = plt.subplot(3, 3, i + 1)\n",
        "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
        "    plt.title(class_names[labels[i]])\n",
        "    plt.axis(\"off\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "id": "fasPiATEkMXp",
        "outputId": "5819769c-b886-49e9-b80c-4f312e40d98f"
      },
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-99-f603bc4bf8c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"uint8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__index__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1065\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1066\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__index__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1067\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__index__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conferindo \"shape\" - ou formato \n",
        "for image_batch, labels_batch in train_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6wwAyrKbkQf7",
        "outputId": "1fd926ca-0d5d-4a90-8433-8f7cfe86abab"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 180, 180, 3)\n",
            "(32, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Conferindo \"shape\" - ou formato \n",
        "for image_batch, labels_batch in val_ds:\n",
        "  print(image_batch.shape)\n",
        "  print(labels_batch.shape)\n",
        "  break"
      ],
      "metadata": {
        "id": "OmVSAVVxMXEk",
        "outputId": "71bb4ad4-7ed5-4763-deaa-9cd347786e08",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(32, 180, 180, 3)\n",
            "(32, 22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing Model"
      ],
      "metadata": {
        "id": "KTIfFsT5kxeB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as K"
      ],
      "metadata": {
        "id": "BHyNGzaAkvMe"
      },
      "execution_count": 102,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_m = K.Input(shape=(180, 180, 3))\n",
        "res_model = K.applications.ResNet50(include_top=False,\n",
        "                                    weights=\"imagenet\",\n",
        "                                    input_tensor=input_m)"
      ],
      "metadata": {
        "id": "uF03vaBt3LHe"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in res_model.layers[:143]:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "jfiaQU6i3aeb"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, layer in enumerate(res_model.layers):\n",
        "  print(i, layer.name, \"-\", layer.trainable)"
      ],
      "metadata": {
        "id": "7lXhcgoT3iaV",
        "outputId": "19b9d06b-6560-448f-d3cb-6ffa9d331bb8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 input_2 - False\n",
            "1 conv1_pad - False\n",
            "2 conv1_conv - False\n",
            "3 conv1_bn - False\n",
            "4 conv1_relu - False\n",
            "5 pool1_pad - False\n",
            "6 pool1_pool - False\n",
            "7 conv2_block1_1_conv - False\n",
            "8 conv2_block1_1_bn - False\n",
            "9 conv2_block1_1_relu - False\n",
            "10 conv2_block1_2_conv - False\n",
            "11 conv2_block1_2_bn - False\n",
            "12 conv2_block1_2_relu - False\n",
            "13 conv2_block1_0_conv - False\n",
            "14 conv2_block1_3_conv - False\n",
            "15 conv2_block1_0_bn - False\n",
            "16 conv2_block1_3_bn - False\n",
            "17 conv2_block1_add - False\n",
            "18 conv2_block1_out - False\n",
            "19 conv2_block2_1_conv - False\n",
            "20 conv2_block2_1_bn - False\n",
            "21 conv2_block2_1_relu - False\n",
            "22 conv2_block2_2_conv - False\n",
            "23 conv2_block2_2_bn - False\n",
            "24 conv2_block2_2_relu - False\n",
            "25 conv2_block2_3_conv - False\n",
            "26 conv2_block2_3_bn - False\n",
            "27 conv2_block2_add - False\n",
            "28 conv2_block2_out - False\n",
            "29 conv2_block3_1_conv - False\n",
            "30 conv2_block3_1_bn - False\n",
            "31 conv2_block3_1_relu - False\n",
            "32 conv2_block3_2_conv - False\n",
            "33 conv2_block3_2_bn - False\n",
            "34 conv2_block3_2_relu - False\n",
            "35 conv2_block3_3_conv - False\n",
            "36 conv2_block3_3_bn - False\n",
            "37 conv2_block3_add - False\n",
            "38 conv2_block3_out - False\n",
            "39 conv3_block1_1_conv - False\n",
            "40 conv3_block1_1_bn - False\n",
            "41 conv3_block1_1_relu - False\n",
            "42 conv3_block1_2_conv - False\n",
            "43 conv3_block1_2_bn - False\n",
            "44 conv3_block1_2_relu - False\n",
            "45 conv3_block1_0_conv - False\n",
            "46 conv3_block1_3_conv - False\n",
            "47 conv3_block1_0_bn - False\n",
            "48 conv3_block1_3_bn - False\n",
            "49 conv3_block1_add - False\n",
            "50 conv3_block1_out - False\n",
            "51 conv3_block2_1_conv - False\n",
            "52 conv3_block2_1_bn - False\n",
            "53 conv3_block2_1_relu - False\n",
            "54 conv3_block2_2_conv - False\n",
            "55 conv3_block2_2_bn - False\n",
            "56 conv3_block2_2_relu - False\n",
            "57 conv3_block2_3_conv - False\n",
            "58 conv3_block2_3_bn - False\n",
            "59 conv3_block2_add - False\n",
            "60 conv3_block2_out - False\n",
            "61 conv3_block3_1_conv - False\n",
            "62 conv3_block3_1_bn - False\n",
            "63 conv3_block3_1_relu - False\n",
            "64 conv3_block3_2_conv - False\n",
            "65 conv3_block3_2_bn - False\n",
            "66 conv3_block3_2_relu - False\n",
            "67 conv3_block3_3_conv - False\n",
            "68 conv3_block3_3_bn - False\n",
            "69 conv3_block3_add - False\n",
            "70 conv3_block3_out - False\n",
            "71 conv3_block4_1_conv - False\n",
            "72 conv3_block4_1_bn - False\n",
            "73 conv3_block4_1_relu - False\n",
            "74 conv3_block4_2_conv - False\n",
            "75 conv3_block4_2_bn - False\n",
            "76 conv3_block4_2_relu - False\n",
            "77 conv3_block4_3_conv - False\n",
            "78 conv3_block4_3_bn - False\n",
            "79 conv3_block4_add - False\n",
            "80 conv3_block4_out - False\n",
            "81 conv4_block1_1_conv - False\n",
            "82 conv4_block1_1_bn - False\n",
            "83 conv4_block1_1_relu - False\n",
            "84 conv4_block1_2_conv - False\n",
            "85 conv4_block1_2_bn - False\n",
            "86 conv4_block1_2_relu - False\n",
            "87 conv4_block1_0_conv - False\n",
            "88 conv4_block1_3_conv - False\n",
            "89 conv4_block1_0_bn - False\n",
            "90 conv4_block1_3_bn - False\n",
            "91 conv4_block1_add - False\n",
            "92 conv4_block1_out - False\n",
            "93 conv4_block2_1_conv - False\n",
            "94 conv4_block2_1_bn - False\n",
            "95 conv4_block2_1_relu - False\n",
            "96 conv4_block2_2_conv - False\n",
            "97 conv4_block2_2_bn - False\n",
            "98 conv4_block2_2_relu - False\n",
            "99 conv4_block2_3_conv - False\n",
            "100 conv4_block2_3_bn - False\n",
            "101 conv4_block2_add - False\n",
            "102 conv4_block2_out - False\n",
            "103 conv4_block3_1_conv - False\n",
            "104 conv4_block3_1_bn - False\n",
            "105 conv4_block3_1_relu - False\n",
            "106 conv4_block3_2_conv - False\n",
            "107 conv4_block3_2_bn - False\n",
            "108 conv4_block3_2_relu - False\n",
            "109 conv4_block3_3_conv - False\n",
            "110 conv4_block3_3_bn - False\n",
            "111 conv4_block3_add - False\n",
            "112 conv4_block3_out - False\n",
            "113 conv4_block4_1_conv - False\n",
            "114 conv4_block4_1_bn - False\n",
            "115 conv4_block4_1_relu - False\n",
            "116 conv4_block4_2_conv - False\n",
            "117 conv4_block4_2_bn - False\n",
            "118 conv4_block4_2_relu - False\n",
            "119 conv4_block4_3_conv - False\n",
            "120 conv4_block4_3_bn - False\n",
            "121 conv4_block4_add - False\n",
            "122 conv4_block4_out - False\n",
            "123 conv4_block5_1_conv - False\n",
            "124 conv4_block5_1_bn - False\n",
            "125 conv4_block5_1_relu - False\n",
            "126 conv4_block5_2_conv - False\n",
            "127 conv4_block5_2_bn - False\n",
            "128 conv4_block5_2_relu - False\n",
            "129 conv4_block5_3_conv - False\n",
            "130 conv4_block5_3_bn - False\n",
            "131 conv4_block5_add - False\n",
            "132 conv4_block5_out - False\n",
            "133 conv4_block6_1_conv - False\n",
            "134 conv4_block6_1_bn - False\n",
            "135 conv4_block6_1_relu - False\n",
            "136 conv4_block6_2_conv - False\n",
            "137 conv4_block6_2_bn - False\n",
            "138 conv4_block6_2_relu - False\n",
            "139 conv4_block6_3_conv - False\n",
            "140 conv4_block6_3_bn - False\n",
            "141 conv4_block6_add - False\n",
            "142 conv4_block6_out - False\n",
            "143 conv5_block1_1_conv - True\n",
            "144 conv5_block1_1_bn - True\n",
            "145 conv5_block1_1_relu - True\n",
            "146 conv5_block1_2_conv - True\n",
            "147 conv5_block1_2_bn - True\n",
            "148 conv5_block1_2_relu - True\n",
            "149 conv5_block1_0_conv - True\n",
            "150 conv5_block1_3_conv - True\n",
            "151 conv5_block1_0_bn - True\n",
            "152 conv5_block1_3_bn - True\n",
            "153 conv5_block1_add - True\n",
            "154 conv5_block1_out - True\n",
            "155 conv5_block2_1_conv - True\n",
            "156 conv5_block2_1_bn - True\n",
            "157 conv5_block2_1_relu - True\n",
            "158 conv5_block2_2_conv - True\n",
            "159 conv5_block2_2_bn - True\n",
            "160 conv5_block2_2_relu - True\n",
            "161 conv5_block2_3_conv - True\n",
            "162 conv5_block2_3_bn - True\n",
            "163 conv5_block2_add - True\n",
            "164 conv5_block2_out - True\n",
            "165 conv5_block3_1_conv - True\n",
            "166 conv5_block3_1_bn - True\n",
            "167 conv5_block3_1_relu - True\n",
            "168 conv5_block3_2_conv - True\n",
            "169 conv5_block3_2_bn - True\n",
            "170 conv5_block3_2_relu - True\n",
            "171 conv5_block3_3_conv - True\n",
            "172 conv5_block3_3_bn - True\n",
            "173 conv5_block3_add - True\n",
            "174 conv5_block3_out - True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = K.models.Sequential()\n",
        "model.add(res_model)\n",
        "model.add(K.layers.Flatten())\n",
        "model.add(K.layers.Dense(128, activation='relu'))\n",
        "model.add(K.layers.Dropout(0.5))\n",
        "model.add(K.layers.Dense(64, activation= 'relu'))\n",
        "model.add(K.layers.Dropout(0.5))\n",
        "model.add(K.layers.Dense(22, activation='softmax'))"
      ],
      "metadata": {
        "id": "WwzpHvo-3uVd"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_point = K.callbacks.ModelCheckpoint(filepath=\"paradinha_modelo.h5\",\n",
        "                                          monitor=\"val_acc\",\n",
        "                                          mode=\"max\",\n",
        "                                          save_best_only=True)"
      ],
      "metadata": {
        "id": "kS_K-wbD395i"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=K.optimizers.RMSprop(lr=2e-5),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_ds, batch_size=32, epochs=20, verbose=1,\n",
        "                    validation_data=val_ds,\n",
        "                    callbacks=[check_point])\n",
        "\n",
        "model.summary()\n",
        "model.save('modelo1.h5')"
      ],
      "metadata": {
        "id": "GLUnwDiR4Kqy",
        "outputId": "4a9d5b7f-a71e-4467-a19a-8f026ceb166c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 110,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizers/optimizer_v2/rmsprop.py:135: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(RMSprop, self).__init__(name, **kwargs)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - ETA: 0s - loss: 3.4956 - accuracy: 0.0841"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 439s 8s/step - loss: 3.4956 - accuracy: 0.0841 - val_loss: 2.8338 - val_accuracy: 0.1841\n",
            "Epoch 2/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 2.9220 - accuracy: 0.1381"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 425s 8s/step - loss: 2.9220 - accuracy: 0.1381 - val_loss: 2.5696 - val_accuracy: 0.3227\n",
            "Epoch 3/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 2.7239 - accuracy: 0.1915"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 422s 8s/step - loss: 2.7239 - accuracy: 0.1915 - val_loss: 2.3066 - val_accuracy: 0.4409\n",
            "Epoch 4/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 2.5627 - accuracy: 0.2261"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 421s 8s/step - loss: 2.5627 - accuracy: 0.2261 - val_loss: 2.0783 - val_accuracy: 0.5136\n",
            "Epoch 5/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 2.4188 - accuracy: 0.2693"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 419s 8s/step - loss: 2.4188 - accuracy: 0.2693 - val_loss: 1.9390 - val_accuracy: 0.5591\n",
            "Epoch 6/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 2.2054 - accuracy: 0.3392"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 422s 8s/step - loss: 2.2054 - accuracy: 0.3392 - val_loss: 1.7251 - val_accuracy: 0.6523\n",
            "Epoch 7/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 2.1161 - accuracy: 0.3540"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 413s 8s/step - loss: 2.1161 - accuracy: 0.3540 - val_loss: 1.6298 - val_accuracy: 0.6523\n",
            "Epoch 8/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 1.9915 - accuracy: 0.3960"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 419s 8s/step - loss: 1.9915 - accuracy: 0.3960 - val_loss: 1.4722 - val_accuracy: 0.7045\n",
            "Epoch 9/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 1.8742 - accuracy: 0.4239"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 420s 8s/step - loss: 1.8742 - accuracy: 0.4239 - val_loss: 1.3248 - val_accuracy: 0.7136\n",
            "Epoch 10/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 1.8028 - accuracy: 0.4466"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 415s 8s/step - loss: 1.8028 - accuracy: 0.4466 - val_loss: 1.1978 - val_accuracy: 0.7568\n",
            "Epoch 11/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 1.7453 - accuracy: 0.4733"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 420s 8s/step - loss: 1.7453 - accuracy: 0.4733 - val_loss: 1.1326 - val_accuracy: 0.7727\n",
            "Epoch 12/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 1.6877 - accuracy: 0.4767"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 415s 8s/step - loss: 1.6877 - accuracy: 0.4767 - val_loss: 1.0681 - val_accuracy: 0.7795\n",
            "Epoch 13/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 1.6072 - accuracy: 0.5119"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 424s 8s/step - loss: 1.6072 - accuracy: 0.5119 - val_loss: 0.9991 - val_accuracy: 0.7932\n",
            "Epoch 14/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 1.5019 - accuracy: 0.5420"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 417s 8s/step - loss: 1.5019 - accuracy: 0.5420 - val_loss: 0.9306 - val_accuracy: 0.8136\n",
            "Epoch 15/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 1.4008 - accuracy: 0.5568"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "55/55 [==============================] - 419s 8s/step - loss: 1.4008 - accuracy: 0.5568 - val_loss: 0.8693 - val_accuracy: 0.8205\n",
            "Epoch 16/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 1.3099 - accuracy: 0.5869"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "55/55 [==============================] - 416s 8s/step - loss: 1.3099 - accuracy: 0.5869 - val_loss: 0.8178 - val_accuracy: 0.8136\n",
            "Epoch 17/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 1.3477 - accuracy: 0.5778"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 419s 8s/step - loss: 1.3477 - accuracy: 0.5778 - val_loss: 0.8239 - val_accuracy: 0.8068\n",
            "Epoch 18/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 1.3030 - accuracy: 0.6006"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 415s 8s/step - loss: 1.3030 - accuracy: 0.6006 - val_loss: 0.8092 - val_accuracy: 0.8045\n",
            "Epoch 19/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 1.2341 - accuracy: 0.6335"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 409s 7s/step - loss: 1.2341 - accuracy: 0.6335 - val_loss: 0.7725 - val_accuracy: 0.8227\n",
            "Epoch 20/20\n",
            "55/55 [==============================] - ETA: 0s - loss: 1.1885 - accuracy: 0.6330"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Can save best model only with val_acc available, skipping.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r55/55 [==============================] - 420s 8s/step - loss: 1.1885 - accuracy: 0.6330 - val_loss: 0.7285 - val_accuracy: 0.8409\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 6, 6, 2048)        23587712  \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 73728)             0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 128)               9437312   \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 22)                1430      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33,034,710\n",
            "Trainable params: 24,422,998\n",
            "Non-trainable params: 8,611,712\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3.9.0 32-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "5b76773ed08602836185ee19b6a28a35c7b52ddebf734b2b96c5aebf91a1aa12"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}